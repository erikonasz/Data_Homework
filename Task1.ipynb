{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task #1: use implemented map-reduce framework for aggregation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having in mind data/clicks dataset with \"date\" column, count how many clicks there were for each date and write the results to data/total_clicks dataset with \"date\" and \"count\" columns."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import glob\n",
    "from collections import defaultdict\n",
    "import os\n",
    "import concurrent.futures\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to read multiple CSV files and store their data in a list.\n",
    "def read_csv(files):\n",
    "    data = []\n",
    "    for file in files:\n",
    "        with open(file, 'r') as csvfile:\n",
    "            reader = csv.DictReader(csvfile) # Read CSV file as dictionary, so that we can get value for each row\n",
    "            for row in reader:\n",
    "                data.append(row)\n",
    "    return data\n",
    "\n",
    "# Create a new directory for the output\n",
    "os.makedirs('data/total_clicks', exist_ok=True)\n",
    "\n",
    "# Function to map the input data to a list of dictionaries with keys and values\n",
    "def mapper(clicks):\n",
    "    result = []\n",
    "    for click in clicks: # Loop through each click in the input list\n",
    "        result.append({'key': click['date'], 'value': click}) # Append a dictionary with 'key' as the date and 'value' as the click\n",
    "    return result\n",
    "\n",
    "# Function to group the mapped data by key (date)\n",
    "def group_by_key(mapped_data):\n",
    "    result = defaultdict(list) # Initialize a defaultdict with lists as default values\n",
    "    for item in mapped_data:\n",
    "        result[item['key']].append(item['value']) # Append (add) the item's value to the list associated with its key\n",
    "    return result\n",
    "\n",
    "# Function to reduce the grouped data by counting the values associated with each key\n",
    "def reducer(key, values):\n",
    "    return {'date': key, 'count': len(values)} # Return a dictionary with the date and the count of values\n",
    "\n",
    "# Function to perform the MapReduce operation on the input files\n",
    "def map_reduce(files):\n",
    "    data = read_csv(files) # Read the input CSV files and store their data in a list\n",
    "    mapped_data = mapper(data) # Map the input data to a list of dictionaries with keys and values\n",
    "    grouped_data = group_by_key(mapped_data) # Group the mapped data by key (date)\n",
    "    \n",
    "    result = [] #Empty list to store the reduced data\n",
    "    for key, values in grouped_data.items():\n",
    "        reduced_item = reducer(key, values) # Reduce the values by counting them\n",
    "        result.append(reduced_item) # Append (add) the reduced item to the result list\n",
    "    \n",
    "    return result\n",
    "\n",
    "def write_output(output_file, data, fieldnames=None):\n",
    "    if not data:\n",
    "        print(\"No data to write.\")\n",
    "        return\n",
    "\n",
    "    if fieldnames is None:\n",
    "        fieldnames = ['date', 'user_id', 'click_target']\n",
    "\n",
    "    sorted_data = sorted(data, key=lambda x: x['date'])\n",
    "\n",
    "    with open(output_file, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        writer.writerows(sorted_data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we output the results to a new .csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the input and output file paths\n",
    "input_files = glob.glob(os.path.join('data/clicks', '*.csv'))\n",
    "output_file = 'data/total_clicks/clicks_per_day.csv'\n",
    "\n",
    "# Call the \"map_reduce\" function on the input files to get the total clicks per day\n",
    "result = map_reduce(input_files)\n",
    "\n",
    "#Lastly, write the results to the output file.\n",
    "write_output(output_file, result, fieldnames=['date', 'count'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data has to be processed in parallel, e.g. by using threads."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can start by adding function map_reduce_parallel to perform MapReduce operation with parallel processing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_reduce_parallel(files):\n",
    "    data = read_csv(files)  # Read the input CSV files and store their data in a list\n",
    "    mapped_data = [] #create new empty list\n",
    "    \n",
    "    # Helper function to divide data into chunks\n",
    "    def chunks(lst, n):\n",
    "        for i in range(0, len(lst), n):\n",
    "            yield lst[i:i + n]\n",
    "\n",
    "    # Divide data into smaller chunks for parallel processing\n",
    "    chunk_size = 10 # Adjustable.  \n",
    "    data_chunks = list(chunks(data, chunk_size))\n",
    "\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor: # Create a ThreadPoolExecutor instance and use it as a context manager named executor\n",
    "        # For each data_chunk in the data list, submit the mapper function to the executor with the data_chunk as its argument. \n",
    "        futures = [executor.submit(mapper, data_chunk) for data_chunk in data_chunks]\n",
    "        for future in concurrent.futures.as_completed(futures):\n",
    "            # For each completed Future, call the result() method to retrieve the result of the mapper function (a list of dictionaries)\n",
    "            mapped_data.extend(future.result()) \n",
    "\n",
    "        grouped_data = group_by_key(mapped_data)  # Group the mapped data by key (date)\n",
    "        result = []  # Empty list to store the reduced data\n",
    "    # Parallel processing for the reducer function\n",
    "\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        # For each key-value pair in the grouped_data dictionary, submit the reducer function to the executor with the key and values as its arguments.\n",
    "        futures = [executor.submit(reducer, key, values) for key, values in grouped_data.items()] \n",
    "        for future in concurrent.futures.as_completed(futures): # Iterate over the Future objects in the futures list as they complete.\n",
    "            result.append(future.result())\n",
    "\n",
    "    return result\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_folder = 'data/clicks'\n",
    "files = [os.path.join(input_folder, file) for file in os.listdir(input_folder) if file.endswith('.csv')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_data = map_reduce_parallel(files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = 'data/total_clicks/output.csv'\n",
    "write_output(output_file, result, fieldnames=['date', 'count'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The file is now accessible Data/Clicks/total_clicks/output.csv"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code first reads the CSV files and divides the data into smaller chunks. It then processes the data in parallel using a ThreadPoolExecutor, applying the mapper function to each chunk. Afterward, it groups the mapped data by key (date) and processes it in parallel again, applying the reducer function to each group. Finally, the reduced data is written to an output file."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
